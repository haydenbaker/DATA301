{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, altair as alt\n",
    "import numpy as np, seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "%matplotlib inline\n",
    "\n",
    "cc_df = pd.read_csv(\"combined_results.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df[[\"Place_Section\", \"float_Time_Section\"]].hist(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are histograms of our most important features, and it appears that they are right skewed. We will log-transform them in order to make the distributions more normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df.Place_Section = np.log10(cc_df.Place_Section)\n",
    "cc_df.float_Time_Section = np.log10(cc_df.float_Time_Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df[[\"Place_Section\", \"float_Time_Section\"]].hist(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models tend to (empirically) favor normally distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = (cc_df[[\"Division\", \"Grade\", \"Place_Section\", \"float_Time_Section\", \"Year\", \"Sex\"]]\n",
    "          .to_dict(orient=\"records\"))\n",
    "y = cc_df[\"float_Time_State\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(X, y, test_size=.2))\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "scaler = RobustScaler()\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "pipeline = Pipeline([(\"vectorizer\", vec), \n",
    "                     (\"scaler\", scaler), \n",
    "                     (\"model\", model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to set an initial benchmark by using the very basic K-Nearest Neighbors model. We will then use these results to compare the model we end up choosing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "cv_scores = []\n",
    "test_scores = []\n",
    "ks = range(1, 10)\n",
    "\n",
    "for k in ks:\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    pipeline = Pipeline([(\"vectorizer\", vec), \n",
    "                     (\"scaler\", scaler), \n",
    "                     (\"model\", model)])\n",
    "    \n",
    "    cv_scores.append(-cross_val_score(pipeline, X_train, y_train, cv=5, scoring=\"neg_mean_squared_error\").mean())\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    test_scores.append(mean_squared_error(pipeline.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame({\"cross-val\": cv_scores, \"test\": test_scores}, index=ks).plot()\n",
    "min_k = np.argmin(test_scores)\n",
    "plt.title(\"KNearest Optimal K\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.text(x=5, y=0.72, s=\"$Test$ = {:3.2f}\".format(test_scores[min_k]))\n",
    "plt.text(x=5, y=0.7, s=\"$CV$ = {:3.2f}\".format(cv_scores[min_k]))\n",
    "plt.vlines(x=min_k + 1, ymin=test_scores[min_k], ymax=cv_scores[min_k], colors=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our graph, we can see that our initial results a pretty promising. We can observe the optimal value of K signalled by the red line connecting the curves. The lowest K is chosen from the lowest observed test error. With regards to the results, a mean squared error of `0.5` implies that our model's predictions of the \"State Time\" (float) were typically off by roughly `0.7` (square-root of the MSE) of a minute, or `42` seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model = GradientBoostingRegressor(min_samples_leaf=3)\n",
    "pipeline = Pipeline([(\"vectorizer\", vec), \n",
    "                     (\"scaler\", scaler), \n",
    "                     (\"model\", model)])\n",
    "    \n",
    "cv_score = (-cross_val_score(pipeline, X_train, y_train, cv=5, scoring=\"neg_mean_squared_error\").mean())\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "test_score = (mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gradient Boosting model was chosen because of it's ensemble-design, and has historically been a powerful model used in learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test = {:3.2f}\".format(test_score))\n",
    "print(\"CV = {:3.2f}\".format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the results, a Gradient Boosting model is better for our problem (approximately 20-25% lower error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"True\": y_test, \"Predicted\": y_pred})\n",
    "sns.jointplot(data=results, x=\"True\", y=\"Predicted\",\n",
    "              ci=100, kind='reg',\n",
    "              joint_kws={'line_kws':{'color':'orange'}})\n",
    "plt.text(x=15, y=25, s=\"$R^2$ = {:3.2f}\".format(pipeline.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our plot shows us the distribution of test values (True), and the predicted values (Predicted) from our model. Our $R^2$ (coefficient of determination) is high, which implies that our model is able to predict values that are very close to the true values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
